
//					Copyright Gavin Band 2008 - 2012.
// Distributed under the Boost Software License, Version 1.0.
//		(See accompanying file LICENSE_1_0.txt or copy at
//					http://www.boost.org/LICENSE_1_0.txt)

#include "../package_revision_autogenerated.hpp"

//#include <unordered_map>
//#include <unordered_set>
//#include <set>
#include <memory>
#include <mutex>
#include <algorithm>
//#include <boost/ptr_container/ptr_vector.hpp>
#include <boost/filesystem.hpp>
#include <boost/noncopyable.hpp>
#include <boost/timer.hpp>
//#include <boost/lockfree/queue.hpp>
#include <chrono>
#include <thread>

// seqlib
#include "SeqLib/RefGenome.h"
#include "SeqLib/BamReader.h"
#include "SeqLib/GenomicRegionCollection.h"
#include "SeqLib/GenomicRegion.h"
//#include "SeqLib/BWAWrapper.h"

namespace seqlib = SeqLib;
// namespace bt = BamTools ;

#include "appcontext/appcontext.hpp"
#include "genfile/GenomePositionRange.hpp"
#include "genfile/FileUtils.hpp"
#include "genfile/string_utils/string_utils.hpp"
#include "genfile/string_utils/slice.hpp"
#include "genfile/Error.hpp"
#include "genfile/kmer/KmerHashIterator.hpp"
#include "statfile/BuiltInTypeStatSink.hpp"

#include "jellyfish/jellyfish.hpp"

#include "parallel_hashmap/phmap.h"
#include "parallel_hashmap/meminfo.h"
#include "concurrentqueue/concurrentqueue.h"

/*
#include <sys/types.h>
#include <sys/sysinfo.h>
*/

#define DEBUG 0

namespace globals {
	std::string const program_name = "classify-kmers" ;
	std::string const program_version = package_version ;
	std::string const program_revision = std::string( package_revision ).substr( 0, 7 ) ;
}

namespace {
	typedef phmap::parallel_flat_hash_map< uint64_t, uint16_t > ParallelHashMap ;
	struct identity_hash {
		std::size_t operator()( uint64_t const value ) { return value ; }
	} ;
	typedef phmap::flat_hash_set<uint64_t> FlatHashSet ;

	typedef phmap::parallel_flat_hash_set<
		uint64_t,
		phmap::priv::hash_default_hash<uint64_t>,
		phmap::priv::hash_default_eq<uint64_t>,
		phmap::priv::Allocator<uint64_t>,
		5, // 2^(this number) of submaps
		std::mutex
	> ParallelFlatHashSet ;

	typedef jellyfish::cooperative::hash_counter<jellyfish::mer_dna> JellyfishHashMap ; 

	// HashSet in actual use.
	//typedef FlatHashSet HashSet ;
	typedef ParallelFlatHashSet HashSet ;

	typedef moodycamel::ConcurrentQueue< uint64_t > Queue ;

	template< typename HashMap >
	void insert_kmer_threaded(
		std::size_t const k,
		Queue* queue,
		HashMap* result,
		std::size_t const thread_index,
		std::atomic< int >* quit
	) {
#if DEBUG
		std::cerr << "(thread " << thread_index << "): Starting...\n" ;
#endif	
		std::size_t count = 0 ;
		uint64_t elt ;
		while( !(*quit) ) {
#if DEBUG > 1
			std::cerr << "!! (" << thread_index << ", " << std::this_thread::get_id() << "): " << queue << ".\n" ;
#endif
			bool popped = queue->try_dequeue( elt ) ;
#if DEBUG > 1
			std::cerr
				<< "(thread " << thread_index << ") "
				<< "!! " << ( popped ? "popped" : "nothing to pop" )
				<< ", queue approx size = " << queue->size_approx()
				<< ".\n" ;
#endif
			if( popped ) {
				result->insert( elt ) ;
				++count ;
				if( (count & 0xFFFFFFF) == 0 ) {
					std::cerr << "(thread " << thread_index << ") ++ Added " << count << " kmers.\n" ;
				}
			} else {
				// nothing to pop, sleep to allow queue to fill.
				std::this_thread::sleep_for( std::chrono::microseconds(10) ) ;
			}
		}
#if DEBUG
		std::cerr
			<< "(thread " << thread_index << "): ++ Added "
			<< count << " kmers in total.\n" ;
		std::cerr << "(thread " << thread_index << "): ++ Ending...\n" ;
#endif
	}
	
	struct Read {
		Read() {}

		/*
		Read( Read&& other ) {
			*this = std::move( other ) ;
		}

		Read& operator=( Read&& other ) {
			if( this != &other ) {
				id = std::move( other.id ) ;
				sequence = std::move( other.sequence ) ;
				qualities = std::move( other.qualities ) ;
			}
		}
		*/
		
		Read( Read const& other ):
			id( other.id ),
			sequence( other.sequence ),
			qualities( other.qualities )
		{}

		Read& operator=( Read const& other ) {
			id = other.id ;
			sequence = other.sequence ;
			qualities = other.qualities ;
			return *this ;
		}

		std::size_t length() const { return sequence.size() ; }

		std::string id ;
		std::string sequence ;
		std::string qualities ;
	} ;

	struct ReadResult {
	public:
		ReadResult():
			read(),
			k(0),
			number_of_kmers_at_threshold(0),
			number_of_solid_kmers_at_threshold(0),
			first_solid_kmer_start(0),
			last_solid_kmer_end(0),
			mean_base_quality(0.0),
			number_of_bases_at_q20(0),
			error_positions()
		{}

		ReadResult( ReadResult const& other ):
			read( other.read ),
			k( other.k ),
			number_of_kmers_at_threshold( other.number_of_kmers_at_threshold ),
			number_of_solid_kmers_at_threshold( other.number_of_solid_kmers_at_threshold ),
			first_solid_kmer_start( other.first_solid_kmer_start ),
			last_solid_kmer_end( other.last_solid_kmer_end ),
			mean_base_quality( other.mean_base_quality ),
			number_of_bases_at_q20( other.number_of_bases_at_q20 ),
			error_positions( other.error_positions )
		{}

		ReadResult& operator=( ReadResult const& other ) {
			read = other.read ;
			k = other.k ;
			number_of_kmers_at_threshold = other.number_of_kmers_at_threshold ;
			number_of_solid_kmers_at_threshold = other.number_of_solid_kmers_at_threshold ;
			first_solid_kmer_start = other.first_solid_kmer_start ;
			last_solid_kmer_end = other.last_solid_kmer_end ;
			mean_base_quality = other.mean_base_quality ;
			number_of_bases_at_q20 = other.number_of_bases_at_q20 ;
			error_positions = other.error_positions ;
			return *this ;
		}
	
		std::string const& id() const { return read.id ; }
		std::size_t length() const { return read.sequence.size() ; }
	public:
		Read read ;
		uint64_t k ;
		uint64_t number_of_kmers_at_threshold ;
		uint64_t number_of_solid_kmers_at_threshold ;
		uint64_t first_solid_kmer_start ;
		uint64_t last_solid_kmer_end ;
		double mean_base_quality ;
		uint64_t number_of_bases_at_q20 ;
		std::vector< std::size_t > error_positions ;
	} ;
	
	struct ReadEndMetrics {
		ReadEndMetrics( std::size_t length_to_track ):
			errors( length_to_track, 0 ),
			counts( length_to_track, 0 ),
			A( length_to_track, 0 ),
			C( length_to_track, 0 ),
			G( length_to_track, 0 ),
			T( length_to_track, 0 ),
			qualities( length_to_track, 0.0 )
		{}

		ReadEndMetrics( ReadEndMetrics const& other ):
			errors( other.errors ),
			counts( other.counts ), 
			A( other.A ), 
			C( other.C ), 
			G( other.G ), 
			T( other.T ),
			qualities( other.qualities )
		{} 

		ReadEndMetrics& operator=( ReadEndMetrics const& other ) {
			errors = other.errors ;
			counts = other.counts ;
			A = other.A ;
			C = other.C ;
			G = other.G ; 
			T = other.T ;
			qualities = other.qualities ;
			return *this ;
		}
		
		std::size_t length() const { return errors.size() ; }
		
		std::vector< uint64_t > errors ;
		std::vector< uint64_t > counts ;
		std::vector< uint64_t > A ;
		std::vector< uint64_t > C ;
		std::vector< uint64_t > G ;
		std::vector< uint64_t > T ;
		std::vector< double > qualities ;
	} ;
	
	typedef moodycamel::ConcurrentQueue< Read > ReadQueue ;
	typedef moodycamel::ConcurrentQueue< ReadResult > ReadResultQueue ;
	
	inline int get_quality_from_char( char const c ) {
		return int(c - 33) ;
	}
	

}

struct AssessPositionOptionProcessor: public appcontext::CmdLineOptionProcessor
{
public:
	std::string get_program_name() const { return globals::program_name ; }

	void declare_options( appcontext::OptionProcessor& options ) {
		// Meta-options
		options.set_help_option( "-help" ) ;
		
		options.declare_group( "Input / output file options" ) ;
		options[ "-jf" ]
			.set_description( "Path of jellyfish file to load kmers from." )
			.set_takes_values_until_next_option()
			.set_is_required()
		;

		options[ "-min-kmer-count" ]
			.set_description( "Kmer multiplicity above which kmers will be treated as solid" )
			.set_takes_single_value()
			.set_default_value(5)
		;

		options[ "-reads" ]
			.set_description( "Path of fastq file of reads to load." )
			.set_takes_single_value()
			.set_is_required()
		;

		options[ "-min-base-quality" ]
			.set_description( "Minimum base quality; kmers containing bases below this quality will not be counted." )
			.set_takes_single_value()
			.set_default_value(0)
		;

		options[ "-o" ]
			.set_description( "Path of output file." )
			.set_takes_single_value()
			.set_default_value( "-" ) ;

		options[ "-op" ]
			.set_description( "Path of position-in-read output file." )
			.set_takes_single_value()
			.set_default_value( "-" ) ;
		
		options.declare_group( "Algorithm options" ) ;
		options[ "-max-kmers" ]
			.set_description( "Only read a maximum of this many kmers.  This is useful for testing." )
			.set_takes_single_value()
			.set_default_value( std::numeric_limits< uint32_t >::max() )
		;
		options[ "-read-end-length" ]
			.set_description( "Length at end of each read to track errors in" )
			.set_takes_single_value()
			.set_default_value( 1000 )
		;

		options.declare_group( "Other options" ) ;
		options[ "-verbose" ]
			.set_description( "Print out details of what is being done." ) ;
		options[ "-threads" ]
			.set_description( "Number of threads to use to load kmers with. " )
			.set_takes_single_value()
			.set_default_value( 1 )
		;
	}
} ;
 
struct ClassifyKmerApplication: public appcontext::ApplicationContext
{
public:
	ClassifyKmerApplication( int argc, char** argv ):
		appcontext::ApplicationContext(
			globals::program_name,
			globals::program_version + ", revision " + globals::program_revision,
			std::auto_ptr< appcontext::OptionProcessor >( new AssessPositionOptionProcessor ),
			argc,
			argv,
			"-log"
		)
	{}
	
	void run() {
		try {
			unsafe_process() ;
		}
		catch( genfile::InputError const& e ) {
			ui().logger() << "\nError (" << e.what() <<"): " << e.format_message() << ".\n" ;
			throw appcontext::HaltProgramWithReturnCode( -1 ) ;
		}
	}

private:

	void unsafe_process() {
		HashSet kmers ;
		std::size_t const number_of_threads = options().get< std::size_t >( "-threads" ) ;
		uint64_t const multiplicity_threshold = options().get_value< uint64_t >( "-min-kmer-count" ) ;
		std::size_t const max_kmers = options().get< std::size_t >( "-max-kmers" ) ;
		int const base_quality_threshold = options().get< std::size_t >( "-min-base-quality" ) ;
		bool const verbose = options().check( "-verbose" ) ;
		std::size_t k = 0 ;

		{
			boost::timer timer ;
			double start_time = timer.elapsed() ;

			k = load_kmers(
				options().get< std::string >( "-jf" ),
				&kmers,
				number_of_threads,
				multiplicity_threshold,
				max_kmers,
				verbose
			) ;

			double end_time = timer.elapsed() ;
			ui().logger()
				<< "++ Ok, "
				<< kmers.size() << " "
				<< k << "-mers loaded in "
				<< (end_time-start_time)
				<< " seconds:\n" ;
		}

		ui().logger() << "++ Total memory usage is:\n" ;
		ui().logger() << "              (process) : " << std::round(spp::GetProcessMemoryUsed()/1000000.0) << "Mb\n" ;
		ui().logger() << "\n" ;
		{
			ui().logger() << "++ Inspecting reads from \"" << options().get< std::string >( "-reads" ) << "\"...\n" ;
			boost::timer timer ;
			double start_time = timer.elapsed() ;
			statfile::BuiltInTypeStatSink::UniquePtr
				position_sink = statfile::BuiltInTypeStatSink::open( options().get< std::string >( "-op" ) ) ;

			statfile::BuiltInTypeStatSink::UniquePtr
				sink = statfile::BuiltInTypeStatSink::open( options().get< std::string >( "-o" ) ) ;

			std::auto_ptr< std::istream >
				fastq = genfile::open_text_file_for_input( options().get< std::string >( "-reads" ) ) ;
		
			std::size_t number_of_reads = process_reads(
				*fastq,
				kmers,
				k,
				base_quality_threshold,
				*sink,
				*position_sink
			) ;
			double end_time = timer.elapsed() ;
			ui().logger() << "++ Ok, processed " << number_of_reads << " reads in " << (end_time - start_time) << " seconds.\n" ;
		}
	}

	std::size_t load_kmers(
		std::string const& jf_filename,
		HashSet* result,
		std::size_t number_of_threads,
		uint64_t multiplicity_threshold = 0,
		std::size_t max_kmers = std::numeric_limits< std::size_t >::max(),
		bool verbose = false
	) {
		std::ifstream ifs( jf_filename ) ;
		jellyfish::file_header header( ifs ) ;
		std::size_t const k = header.key_len() / 2 ;
		if(verbose) {
			ui().logger()
				<< "++ Loaded header from \"" << jf_filename << "\":\n"
				<< "    size: " << header.size() << "\n"
				<< "    nb_hashes: " << header.nb_hashes() << "\n"
				<< "    key_len: " << header.key_len() << "\n"
				<< "          k: " << k << ".\n" ;
		}

		if( header.format() != binary_dumper::format ) {
			throw genfile::BadArgumentError( "ClassifyKmerApplication::unsafe_process()", "-jf", "Expected a binary-format jellyfish count file." ) ;
		}

		{
			{
				if( number_of_threads < 1 ) {
					throw genfile::BadArgumentError(
						"ClassifyKmerApplication::load_kmers()",
						"number_of_threads",
						"You must supply a value >= 1"
					) ;
				}

				if( (number_of_threads & (number_of_threads - 1) ) != 0 ) {
					throw genfile::BadArgumentError(
						"ClassifyKmerApplication::load_kmers()",
						"number_of_threads",
						"Number of threads must be zero or a power of two."
					) ;
				}

				if( number_of_threads > 32 ) {
					throw genfile::BadArgumentError(
						"ClassifyKmerApplication::load_kmers()",
						"number_of_threads",
						"A maximum of 32 threads are supported."
					) ;
				}
			}
			
			jellyfish::mer_dna::k( k ) ;
			binary_reader reader(ifs, &header);

			{
				std::vector< Queue > queues ;
				std::vector< std::thread > threads ;
				std::atomic< int > quit(0) ;
				ui().logger() << "++ Loading kmers from \"" << jf_filename << "\"\n"
					<< "   ...using " << number_of_threads << " worker threads...\n" ;
			
				// Create worker threads
				for( std::size_t i = 0; i < number_of_threads; ++i ) {
					queues.push_back( Queue( 32768 ) ) ;
					if( verbose ) {
						ui().logger() << "!! Created queue " << i << " at (" << &(queues.back()) << ").\n" ;
					}
				}
				for( std::size_t i = 0; i < number_of_threads; ++i ) {
					threads.push_back(
						std::thread(
							insert_kmer_threaded< HashSet >,
							k,
							&(queues[i]),
							result,
							i,
							&quit
						)
					) ;
					if( verbose ) {
						ui().logger() << "!! Created thread " << i << " at (" << &(threads.back()) << ").\n" ;
					}
				}
				
				// Now read kmers into queues.
				// There is one queue per thread and gets kmers destined for ith
				// hash submap.
				read_into_queues(
					k,
					multiplicity_threshold,
					reader,
					queues,
					*result,
					max_kmers
				) ;
				
				// Wait for it to finish.
				for( std::size_t i = 0; i < number_of_threads; ++i ) {
					while( queues[i].size_approx() > 0 ) {
#if DEBUG
						std::cerr << "++ Queue[" << i << "] size = " << queues[i].size_approx() << ", waiting..." ;
#endif
						std::this_thread::sleep_for( std::chrono::milliseconds(1)) ;
					}
				}

				ui().logger() << "++ Tidying up...\n" ;
				quit = 1 ;
				std::this_thread::sleep_for( std::chrono::milliseconds(1)) ;
				for( std::size_t i = 0; i < number_of_threads; ++i ) {
					threads[i].join() ;
				}
			}
		}
		return k ;
	}
	
	template< typename Iterator >
	void read_into_queues(
		unsigned int const k,
		uint64_t const multiplicity_threshold,
		Iterator it,
		std::vector< Queue >& queues,
		HashSet const& set,
		std::size_t const max_kmers = std::numeric_limits< std::size_t >::max()
	) {
		jellyfish::mer_dna::k( k ) ;
		std::size_t count = 0 ;
		{
			auto progress = ui().get_progress_context( "Loading kmers" ) ;
			while( it.next() && count < max_kmers ) {
				if( it.val() >= multiplicity_threshold ) {
					uint64_t const kmer = it.key().get_bits( 0, 2*k ) ;
					std::size_t const hashvalue = set.hash( kmer ) ;
					std::size_t idx = set.subidx( hashvalue ) ;
#if DEBUG > 1
					std::cerr << "++ kmer: " << genfile::kmer::decode_hash( kmer, k )
						<< ", "
						<< std::hex << kmer << std::dec
						<< ": " << "hashvalue: "
						<< hashvalue << ", idx: " << idx << ".\n" ;
#endif
					std::size_t const queue_index = idx % queues.size() ;
					// Queue& queue = queues[ queue_index ] ;
					Queue& queue = queues[ queue_index ] ;
					while( !queue.try_enqueue( kmer )) {
#if DEBUG > 1
						std::cerr << "-- queue full after " << count << " kmers, sleeping...\n" ;
#endif
						std::this_thread::sleep_for( std::chrono::microseconds(10) ) ;
					}
#if DEBUG > 1
					std::cerr << "++ Wrote " << kmer << " to queue " << queue_index << ".\n" ;
#endif
					progress( ++count ) ;
				}
			}
		}
		ui().logger() << "++ read_into_queue(): Read " << count << " kmers in total.\n" ;
	}
	

	std::size_t process_reads(
		std::istream& input,
		HashSet const& kmers,
		std::size_t k,
		uint64_t base_quality_threshold,
		statfile::BuiltInTypeStatSink& output,
		statfile::BuiltInTypeStatSink& position_output
	) {
		std::size_t const number_of_threads = options().get< std::size_t >( "-threads" ) ;

		auto progress = ui().get_progress_context( "Examining reads" ) ;

		std::size_t count = 0 ;

		std::cerr << "process_read(): constructing " << number_of_threads << " worker threads...\n" ;
		std::vector< std::thread > threads ;
		{
			ReadQueue read_queue( 2048 ) ;
			ReadResultQueue read_result_queue( 32768 ) ;
			std::atomic< int > quit(0) ;

			for( std::size_t i = 0; i < number_of_threads; ++i ) {
				threads.push_back(
					std::thread(
						&ClassifyKmerApplication::analyse_reads_threaded,
						this,
						&read_queue,
						&read_result_queue,
						&kmers,
						k,
						base_quality_threshold,
						i,
						&quit
					)
				) ;
			}
			std::cerr << "process_read(): constructing output thread...\n" ;

			std::size_t const length_to_track_at_read_ends = options().get< std::size_t >( "-read-end-length" ) ;
			threads.push_back(
				std::thread(
					&ClassifyKmerApplication::process_read_results,
					this,
					&read_result_queue,
					&output,
					&position_output,
					length_to_track_at_read_ends,
					&quit
				)
			) ;

			std::cerr << "process_read(): processing reads...\n" ;
			Read read ;
			std::string line ;
			std::size_t l = 0 ;
			while( std::getline( input, line )) {
				switch(l) {
					case 0:
						read.id = line.substr(1,line.size() ) ;
						break ;
					case 1:
						read.sequence = line ;
						break ;
					case 2:
						break ;
					case 3:
						read.qualities = line ;
						break ;
				} ;

				if( (++l) == 4 ) {
					while( !read_queue.try_enqueue( read )) {
#if DEBUG > 1
						std::cerr << "-- queue full after " << count << " reads, sleeping...\n" ;
#endif
						std::this_thread::sleep_for( std::chrono::microseconds(10) ) ;
					}
					
#if DEBUG
					std::cerr << "queued: " << read.id << ".\n" ;
#endif
					++count ;
					progress( count ) ;
					l = 0 ;
				}
			}

			ui().logger() << "++ Done, read " << count << " reads in total.\n" ;

			ui().logger() << "++ Waiting to finish...\n" ;
			while( (read_queue.size_approx() > 0) || (read_result_queue.size_approx() > 0) ) {
				std::this_thread::sleep_for( std::chrono::milliseconds(10)) ;
			}
			std::this_thread::sleep_for( std::chrono::milliseconds(100)) ;
			quit = 1 ;
			std::this_thread::sleep_for( std::chrono::milliseconds(100)) ;
			for( std::size_t i = 0; i < threads.size(); ++i ) {
				threads[i].join() ;
			}
		}

		return count ;
	}
	
	void compute_min_quality(
		genfile::string_utils::slice const& qualities,
		int& min_quality,
		std::size_t& min_quality_at
	) {
		min_quality = std::numeric_limits< int >::max() ;
		for( std::size_t i = 0; i < qualities.size(); ++i ) {
			int quality = get_quality_from_char(qualities[i]) ;
			if( quality < min_quality ) {
				min_quality_at = i ;
				min_quality = quality ;
			}
		}
	}
	
	ReadResult analyse_read(
		Read const& read,
		HashSet const& kmers,
		std::size_t k,
		int const base_quality_threshold
	) {
#if DEBUG
		std::cerr << "analyse_read(): " << read.id << ".\n" ;
#endif
		assert( read.qualities.size() == read.sequence.size() ) ;
		assert( k <= 31 ) ;
		typedef genfile::kmer::KmerHashIterator< std::string::const_iterator > KmerIterator ;

		ReadResult result ;
		result.read = read ;
		result.k = k ;
		KmerIterator kmer_iterator( read.sequence.begin(), read.sequence.end(), k ) ;

		int kmer_min_base_quality = 0 ;
		std::size_t kmer_min_base_quality_at = 0 ;
		double sum_of_base_qualities = 0.0 ;
		uint64_t number_of_bases_at_q20 = 0 ;
		bool have_first = false ;
		
		std::size_t i = 0;

		for(
			;
			(i+k) <= read.length();
			++kmer_iterator, ++i
		) {
			int base_quality = get_quality_from_char(read.qualities[i]) ;
			sum_of_base_qualities += double(base_quality) ;
#if DEBUG > 2
			std::cerr
				<< "!! "
				<< read.id << ": " << i << " bq = "
				<< base_quality << ", " << base_quality_threshold
				<< "; k = " << k 
				<< "... adding\n" ;
#endif
			number_of_bases_at_q20 += ( base_quality >= 20 ) ? 1 : 0 ;

			if( kmer_min_base_quality_at == 0 ) {
				compute_min_quality(
					genfile::string_utils::slice( read.qualities, i, i+k ),
					kmer_min_base_quality,
					kmer_min_base_quality_at
				) ;
			} else {
				--kmer_min_base_quality_at ;
			}
			if( kmer_min_base_quality >= base_quality_threshold ) {
				++(result.number_of_kmers_at_threshold) ;
				uint64_t const hash = kmer_iterator.hash() ;
				uint64_t const reverse_complement_hash = genfile::kmer::reverse_complement( kmer_iterator.hash(), k ) ;
				if( kmers.contains( hash ) || kmers.contains( reverse_complement_hash ) ) {
					if( !have_first ) {
						result.first_solid_kmer_start = i ;  // 0-based, half-closed
						have_first = true ;
					}
					++(result.number_of_solid_kmers_at_threshold) ;
					result.last_solid_kmer_end = i+k ; // 0-based, half-closed
				} else {
					result.error_positions.push_back(i) ;
#if DEBUG
					std::cerr << "!! kmer not in hash:" << kmer_iterator.to_string() << ":\n"
						<< "    (fwd): " << genfile::kmer::decode_hash( hash, k ) << ": " << hash << "\n"
						<< "    (rev): " << genfile::kmer::decode_hash( reverse_complement_hash, k ) << ".\n" ;
#endif
				}
			}
			/*
			if( kmer_iterator.finished() ) {
				++i ;
				break ;
			}
			*/
		}

#if DEBUF	
		std::cerr << "analyse_read(): " << read.id << ": capturing quality metrics...\n" ;
#endif
		// capture remaining bases for base quality metric
		for(
			;
			i < read.length();
			++i
		) {
			int base_quality = get_quality_from_char(read.qualities[i]) ;
#if DEBUG > 2
			std::cerr << "!!! " << read.id << ": " << i << " bq = " << base_quality << "... adding\n" ;
#endif
			sum_of_base_qualities += double(base_quality) ;
			number_of_bases_at_q20 += ( base_quality >= 20 ) ? 1 : 0 ;
		}
		
		result.mean_base_quality = sum_of_base_qualities / result.read.length() ;
		result.number_of_bases_at_q20 = number_of_bases_at_q20 ;
#if DEBUG
		std::cerr << "analyse_read(): " << read.id << ": finished.\n" ;
#endif
		return result ;
	}
	
	void analyse_reads_threaded(
		ReadQueue* read_queue,
		ReadResultQueue* result_queue,
		HashSet const* kmers,
		std::size_t k,
		uint64_t base_quality_threshold,
		std::size_t const thread_index,
		std::atomic< int >* quit
	) {
#if DEBUG
		std::cerr << "(thread " << thread_index << "): Starting...\n" ;
#endif	
		Read read ;
		while( !(*quit) ) {
#if DEBUG > 1
			std::cerr << "!! (" << thread_index << ", " << std::this_thread::get_id() << "): " << queue << ".\n" ;
#endif
			bool popped = read_queue->try_dequeue( read ) ;
			if( popped ) {
#if DEBUG
				std::cerr << "++ Analysing read " << read.id << ".\n" ;
#endif
				ReadResult const result = analyse_read( read, *kmers, k, base_quality_threshold ) ;
				while( !result_queue->try_enqueue( result )) {
					std::this_thread::sleep_for( std::chrono::microseconds(10) ) ;
				}
#if DEBUG
				std::cerr << "++ Analysed read " << read.id << ".\n" ;
#endif
			} else {
				// nothing to pop, sleep to allow queue to fill.
				std::this_thread::sleep_for( std::chrono::microseconds(10) ) ;
			}
		}
	}
	


	void process_read_results(
		ReadResultQueue* result_queue,
		statfile::BuiltInTypeStatSink* output,
		statfile::BuiltInTypeStatSink* read_position_results,
		std::size_t const length_to_track_at_read_ends,
		std::atomic< int >* quit
	) {
		write_per_read_result_header( output ) ;
		
		std::size_t const end_length = length_to_track_at_read_ends ;
		ReadEndMetrics read_start_metrics( end_length ) ;
		ReadEndMetrics read_end_metrics( end_length ) ;

		std::size_t count = 0 ;

		ReadResult result ;
		while( !(*quit) ) {
			bool popped = result_queue->try_dequeue( result ) ;
			if( popped ) {
				process_read_result( result, output ) ;
				if( result.read.length() >= 2 * end_length ) {
					accumulate_read_end_metrics(
						result,
						read_start_metrics,
						read_end_metrics
					) ;
				}
				++count ;
			} else {
				// nothing to pop, sleep to allow queue to fill.
				std::this_thread::sleep_for( std::chrono::microseconds(10) ) ;
			}
		}

		process_per_position_results(
			read_start_metrics,
			read_end_metrics,
			read_position_results
		) ;
	}
	
	void process_read_result(
		ReadResult const& read_result,
		statfile::BuiltInTypeStatSink* output
	) {
		write_per_read_result(
			read_result,
			output
		) ;
	}
	
	void write_per_read_result_header(
		statfile::BuiltInTypeStatSink* output
	) {
		(*output)
			| "read_id"
			| "read_length"
			| "mean_base_quality"
			| "number_of_bases_at_q20"
			| "kmer_k"
			| "number_of_kmers_at_threshold"
			| "number_of_solid_kmers_at_threshold"
			| "first_solid_kmer_start"
			| "last_solid_kmer_end"
		;
	}

	void write_per_read_result(
		ReadResult const& read_result,
		statfile::BuiltInTypeStatSink* output
	) {
#if DEBUG
		std::cerr << "++ write_per_read_result(): " << result.id << ".\n" ;
#endif
		(*output)
			<< read_result.read.id
			<< uint64_t(read_result.read.length())
			<< read_result.mean_base_quality
			<< read_result.number_of_bases_at_q20
			<< read_result.k
			<< read_result.number_of_kmers_at_threshold
			<< read_result.number_of_solid_kmers_at_threshold
			<< (read_result.first_solid_kmer_start+1) 			// convert to 1-based, closed
			<< (read_result.last_solid_kmer_end) 				// 1-based, closed.
			<< statfile::end_row() ;
	}

	void accumulate_read_end_metrics(
		ReadResult const& read_result,
		ReadEndMetrics& read_start_metrics,
		ReadEndMetrics& read_end_metrics
	) {
		// Accumulate where errors lie in the read.
		// We only count reads with at least twice the required length.
		// This way we don't capture end-of-read effects in the start-of-read accounting,
		// and vice-versa.
		std::size_t const end_length = read_start_metrics.length() ;
		for( std::size_t i = 0; i < end_length; ++i ) {
			std::size_t end_of_read_i = read_result.read.length() - end_length + i ;

			// accumulate read counts
			++read_start_metrics.counts[i] ;
			++read_end_metrics.counts[i] ;
			
			// accumulate bases
			switch( read_result.read.sequence[i] ) {
				case 'A':
				case 'a':
					++read_start_metrics.A[i] ;
					break ;
				case 'C':
				case 'c':
					++read_start_metrics.C[i] ;
					break ;
				case 'G':
				case 'g':
					++read_start_metrics.G[i] ;
					break ;
				case 'T':
				case 't':
					++read_start_metrics.T[i] ;
					break ;
				default:
					break ;
			}

			switch( read_result.read.sequence[end_of_read_i] ) {
				case 'A':
				case 'a':
					++read_end_metrics.A[i] ;
					break ;
				case 'C':
				case 'c':
					++read_end_metrics.C[i] ;
					break ;
				case 'G':
				case 'g':
					++read_end_metrics.G[i] ;
					break ;
				case 'T':
				case 't':
					++read_end_metrics.T[i] ;
					break ;
				default:
					break ;
			}
			
			// accumulate mean base qualities - use Welford online algorithm
			// for computing mean base quality.
			read_start_metrics.qualities[i] += get_quality_from_char( read_result.read.qualities[i] ) ;
			read_end_metrics.qualities[i] += get_quality_from_char( read_result.read.qualities[end_of_read_i] ) ;
		}
		
		for( std::size_t i = 0; i < read_result.error_positions.size(); ++i ) {
			std::size_t const pos = read_result.error_positions[i] ;
			// Example:
			//   = =               kmer pos = 1
			// - - - - - - - - - - sequence length = 10
			//  0 1 2 3 4 5 6 7 8 9  
			// [         ]  end_length = 4 capturing 4+k-1 bases
			if( pos < end_length ) {
				++read_start_metrics.errors[pos] ;
			}
			// Example:
			//                 = = kmer pos = 8
			// - - - - - - - - - - sequence length = 10
			// 0 1 2 3 4 5 6 7 8 9  
			//          [         ]  end_length = 4 capturing 4+k-1 bases
			// we need kmers with pos + end_length + k > sequence length
			// and pos maps to pos + (end_length + k - 1) - sequence length
			// e.g. in this case 8+4+2-1-10 = 3.
			if( (pos + end_length + read_result.k ) > read_result.read.length() ) {
				std::size_t const idx = (pos + end_length + read_result.k) - 1 - read_result.read.length() ;
#if DEBUG > 1
				std::cerr
					<<   "        pos: " << pos
					<< "\n end_length: " << end_length 
					<< "\n          k: " << read_result.k
					<< "\n     length: " << read_result.read.length()
					<< "\n      index: " << idx
					<< "\n" ;
#endif
				++read_end_metrics.errors[idx] ;
			}
		}
	}
	
	void process_per_position_results(
		ReadEndMetrics const& read_start_metrics,
		ReadEndMetrics const& read_end_metrics,
		statfile::BuiltInTypeStatSink* read_position_results
	) const {
		(*read_position_results)
			| "start_or_end"
			| "position"
			| "number_of_errors"
			| "number_of_reads"
			| "sum_of_base_qualities"
			| "A"
			| "C"
			| "G"
			| "T"
		;
		for( std::size_t i = 0; i < read_start_metrics.length(); ++i ) {
			(*read_position_results)
				<< "start"
				<< uint64_t(i+1)
				<< read_start_metrics.errors[i]
				<< read_start_metrics.counts[i]
				<< read_start_metrics.qualities[i]
				<< read_start_metrics.A[i]
				<< read_start_metrics.C[i]
				<< read_start_metrics.G[i]
				<< read_start_metrics.T[i]
				<< statfile::end_row() ;
		}
		for( std::size_t i = 0; i < read_end_metrics.length(); ++i ) {
			(*read_position_results)
				<< "end"
				// 0 maps to -end_of_read_errors.size()
				<< (-int64_t(read_end_metrics.length()) + int64_t(i) )
				<< read_end_metrics.errors[i]
				<< read_end_metrics.counts[i]
				<< read_end_metrics.qualities[i]
				<< read_end_metrics.A[i]
				<< read_end_metrics.C[i]
				<< read_end_metrics.G[i]
				<< read_end_metrics.T[i]
				<< statfile::end_row() ;
		}
	}
} ;


int main( int argc, char** argv )
{
	std::ios_base::sync_with_stdio( false ) ;
	try {
		ClassifyKmerApplication app( argc, argv ) ;
		app.run() ;
	}
	catch( appcontext::HaltProgramWithReturnCode const& e ) {
		return e.return_code() ;
	}
	return 0 ;
}

