
//					Copyright Gavin Band 2008 - 2012.
// Distributed under the Boost Software License, Version 1.0.
//		(See accompanying file LICENSE_1_0.txt or copy at
//					http://www.boost.org/LICENSE_1_0.txt)

#include "../package_revision_autogenerated.hpp"

//#include <unordered_map>
//#include <unordered_set>
//#include <set>
#include <memory>
#include <mutex>
#include <algorithm>
//#include <boost/ptr_container/ptr_vector.hpp>
#include <boost/filesystem.hpp>
#include <boost/noncopyable.hpp>
#include <boost/timer.hpp>
//#include <boost/lockfree/queue.hpp>
#include <chrono>
#include <thread>

// seqlib
#include "SeqLib/RefGenome.h"
#include "SeqLib/BamReader.h"
#include "SeqLib/GenomicRegionCollection.h"
#include "SeqLib/GenomicRegion.h"
//#include "SeqLib/BWAWrapper.h"

namespace seqlib = SeqLib;
// namespace bt = BamTools ;

#include "appcontext/appcontext.hpp"
#include "genfile/GenomePositionRange.hpp"
#include "genfile/FileUtils.hpp"
#include "genfile/string_utils/string_utils.hpp"
#include "genfile/string_utils/slice.hpp"
#include "genfile/Error.hpp"
#include "genfile/kmer/KmerHashIterator.hpp"
#include "statfile/BuiltInTypeStatSource.hpp"
#include "statfile/BuiltInTypeStatSink.hpp"

#include "jellyfish/jellyfish.hpp"

#include "parallel_hashmap/phmap.h"
#include "parallel_hashmap/meminfo.h"
#include "concurrentqueue/concurrentqueue.h"

/*
#include <sys/types.h>
#include <sys/sysinfo.h>
*/

#define DEBUG 0

namespace globals {
	std::string const program_name = "classify-kmers" ;
	std::string const program_version = package_version ;
	std::string const program_revision = std::string( package_revision ).substr( 0, 7 ) ;
}

struct ClassifyKmersOptionProcessor: public appcontext::CmdLineOptionProcessor
{
public:
	std::string get_program_name() const { return globals::program_name ; }

	void declare_options( appcontext::OptionProcessor& options ) {
		// Meta-options
		options.set_help_option( "-help" ) ;
		
		options.declare_group( "Input / output file options" ) ;
		options[ "-jf" ]
			.set_description( "Path of jellyfish file to load kmers from." )
			.set_takes_values_until_next_option()
			.set_is_required()
		;

		options[ "-min-kmer-count" ]
			.set_description( "Kmer multiplicity above which kmers will be treated as solid" )
			.set_takes_single_value()
			.set_default_value(5)
		;

		options[ "-reads" ]
			.set_description( "Path of fastq file of reads to load." )
			.set_takes_single_value()
			.set_is_required()
		;

		options[ "-read-tags" ]
			.set_description( "Path of a text file with two (named) columns.  The first column is the "
			" read id, the second is a string value to associate to the read.  Results will be split over the different tag values." )
			.set_takes_single_value()
		;

		options[ "-min-base-quality" ]
			.set_description( "Minimum base quality; kmers containing bases below this quality will not be counted." )
			.set_takes_single_value()
			.set_default_value(0)
		;

		options[ "-or" ]
			.set_description( "Path of per-read output file." )
			.set_takes_single_value()
			.set_default_value( "-" ) ;

		options[ "-op" ]
			.set_description( "Path of per-position-in-read output file." )
			.set_takes_single_value()
			.set_default_value( "-" ) ;
		
		options[ "-oq" ]
			.set_description( "Path of per-base quality output file." )
			.set_takes_single_value()
			.set_default_value( "-" ) ;

		options.declare_group( "Algorithm options" ) ;
		options[ "-max-kmers" ]
			.set_description( "Only read a maximum of this many kmers.  This is useful for testing." )
			.set_takes_single_value()
			.set_default_value( std::numeric_limits< uint32_t >::max() )
		;
		options[ "-read-end-length" ]
			.set_description( "Length at end of each read to track errors in" )
			.set_takes_single_value()
			.set_default_value( 1000 )
		;

		options.declare_group( "Other options" ) ;
		options[ "-verbose" ]
			.set_description( "Print out details of what is being done." ) ;
		options[ "-threads" ]
			.set_description( "Number of threads to use to load kmers with. " )
			.set_takes_single_value()
			.set_default_value( 1 )
		;
	}
} ;

namespace {
	typedef phmap::parallel_flat_hash_map< uint64_t, uint16_t > ParallelHashMap ;
	struct identity_hash {
		std::size_t operator()( uint64_t const value ) { return value ; }
	} ;
	typedef phmap::flat_hash_set<uint64_t> FlatHashSet ;

	typedef phmap::parallel_flat_hash_set<
		uint64_t,
		phmap::priv::hash_default_hash<uint64_t>,
		phmap::priv::hash_default_eq<uint64_t>,
		phmap::priv::Allocator<uint64_t>,
		5, // 2^(this number) of submaps
		std::mutex
	> ParallelFlatHashSet ;

	typedef jellyfish::cooperative::hash_counter<jellyfish::mer_dna> JellyfishHashMap ; 

	// HashSet in actual use.
	//typedef FlatHashSet HashSet ;
	typedef ParallelFlatHashSet HashSet ;

	typedef moodycamel::ConcurrentQueue< uint64_t > Queue ;

	template< typename HashMap >
	void insert_kmer_threaded(
		std::size_t const k,
		Queue* queue,
		HashMap* result,
		std::size_t const thread_index,
		std::atomic< int >* quit
	) {
#if DEBUG
		std::cerr << "(thread " << thread_index << "): Starting...\n" ;
#endif	
		std::size_t count = 0 ;
		uint64_t elt ;
		while( !(*quit) ) {
#if DEBUG > 1
			std::cerr << "!! (" << thread_index << ", " << std::this_thread::get_id() << "): " << queue << ".\n" ;
#endif
			bool popped = queue->try_dequeue( elt ) ;
#if DEBUG > 1
			std::cerr
				<< "(thread " << thread_index << ") "
				<< "!! " << ( popped ? "popped" : "nothing to pop" )
				<< ", queue approx size = " << queue->size_approx()
				<< ".\n" ;
#endif
			if( popped ) {
				result->insert( elt ) ;
				++count ;
				if( (count & 0xFFFFFFF) == 0 ) {
					std::cerr << "(thread " << thread_index << ") ++ Added " << count << " kmers.\n" ;
				}
			} else {
				// nothing to pop, sleep to allow queue to fill.
				std::this_thread::sleep_for( std::chrono::microseconds(10) ) ;
			}
		}
#if DEBUG
		std::cerr
			<< "(thread " << thread_index << "): ++ Added "
			<< count << " kmers in total.\n" ;
		std::cerr << "(thread " << thread_index << "): ++ Ending...\n" ;
#endif
	}
	
	struct Read {
		Read() {}

		/*
		Read( Read&& other ) {
			*this = std::move( other ) ;
		}

		Read& operator=( Read&& other ) {
			if( this != &other ) {
				id = std::move( other.id ) ;
				sequence = std::move( other.sequence ) ;
				qualities = std::move( other.qualities ) ;
			}
		}
		*/
		
		Read( Read const& other ):
			id( other.id ),
			sequence( other.sequence ),
			qualities( other.qualities )
		{}

		Read& operator=( Read const& other ) {
			id = other.id ;
			sequence = other.sequence ;
			qualities = other.qualities ;
			return *this ;
		}

		std::size_t length() const { return sequence.size() ; }

		std::string id ;
		std::string sequence ;
		std::string qualities ;
	} ;

	struct ReadResult {
	public:
		ReadResult():
			read(),
			tag(),
			k(0),
			number_of_kmers_at_threshold(0),
			number_of_solid_kmers_at_threshold(0),
			first_solid_kmer_start(0),
			last_solid_kmer_end(0),
			mean_base_quality(0.0),
			mean_base_quality2(0.0), // accumulated on the error probability scale
			number_of_bases_ge_q(10, 0ul ),
			error_positions(),
			bases_at_q(94,0)
		{}

		ReadResult( ReadResult const& other ):
			read( other.read ),
			tag( other.tag ),
			k( other.k ),
			number_of_kmers_at_threshold( other.number_of_kmers_at_threshold ),
			number_of_solid_kmers_at_threshold( other.number_of_solid_kmers_at_threshold ),
			first_solid_kmer_start( other.first_solid_kmer_start ),
			last_solid_kmer_end( other.last_solid_kmer_end ),
			mean_base_quality( other.mean_base_quality ),
			mean_base_quality2( other.mean_base_quality2 ),
			number_of_bases_ge_q( other.number_of_bases_ge_q ),
			error_positions( other.error_positions ),
			bases_at_q( other.bases_at_q )
		{}

		ReadResult& operator=( ReadResult const& other ) {
			read = other.read ;
			tag = other.tag ;
			k = other.k ;
			number_of_kmers_at_threshold = other.number_of_kmers_at_threshold ;
			number_of_solid_kmers_at_threshold = other.number_of_solid_kmers_at_threshold ;
			first_solid_kmer_start = other.first_solid_kmer_start ;
			last_solid_kmer_end = other.last_solid_kmer_end ;
			mean_base_quality = other.mean_base_quality ;
			mean_base_quality2 = other.mean_base_quality2 ;
			number_of_bases_ge_q = other.number_of_bases_ge_q ;
			error_positions = other.error_positions ;
			bases_at_q = other.bases_at_q ;
			return *this ;
		}
	
		std::string const& id() const { return read.id ; }
		std::size_t length() const { return read.sequence.size() ; }
	public:
		Read read ;
		std::string tag ;
		uint64_t k ;
		uint64_t number_of_kmers_at_threshold ;
		uint64_t number_of_solid_kmers_at_threshold ;
		uint64_t first_solid_kmer_start ;
		uint64_t last_solid_kmer_end ;
		double mean_base_quality ;
		double mean_base_quality2 ;
		std::vector< uint64_t > number_of_bases_ge_q ;
		std::vector< std::size_t > error_positions ;
		std::vector< uint64_t > bases_at_q ;
	} ;
	
	struct ReadEndMetrics {
		ReadEndMetrics( std::size_t length_to_track ):
			errors( length_to_track, 0 ),
			counts( length_to_track, 0 ),
			A( length_to_track, 0 ),
			C( length_to_track, 0 ),
			G( length_to_track, 0 ),
			T( length_to_track, 0 ),
			qualities( length_to_track, 0.0 )
		{}

		ReadEndMetrics( ReadEndMetrics const& other ):
			errors( other.errors ),
			counts( other.counts ), 
			A( other.A ), 
			C( other.C ), 
			G( other.G ), 
			T( other.T ),
			qualities( other.qualities )
		{} 

		ReadEndMetrics& operator=( ReadEndMetrics const& other ) {
			errors = other.errors ;
			counts = other.counts ;
			A = other.A ;
			C = other.C ;
			G = other.G ; 
			T = other.T ;
			qualities = other.qualities ;
			return *this ;
		}
		
		std::size_t length() const { return errors.size() ; }
		
		std::vector< uint64_t > errors ;
		std::vector< uint64_t > counts ;
		std::vector< uint64_t > A ;
		std::vector< uint64_t > C ;
		std::vector< uint64_t > G ;
		std::vector< uint64_t > T ;
		std::vector< double > qualities ;
	} ;
	
	typedef moodycamel::ConcurrentQueue< Read > ReadQueue ;
	typedef moodycamel::ConcurrentQueue< ReadResult > ReadResultQueue ;
	
	inline int get_quality_from_char( char const c ) {
		return int(c - 33) ;
	}
	

}

 
struct ClassifyKmerApplication: public appcontext::ApplicationContext
{
public:
	ClassifyKmerApplication( int argc, char** argv ):
		appcontext::ApplicationContext(
			globals::program_name,
			globals::program_version + ", revision " + globals::program_revision,
			std::auto_ptr< appcontext::OptionProcessor >( new ClassifyKmersOptionProcessor ),
			argc,
			argv,
			"-log"
		),
		m_k(0)
	{}
	
	void run() {
		try {
			unsafe_process() ;
		}
		catch( genfile::InputError const& e ) {
			ui().logger() << "\nError (" << e.what() <<"): " << e.format_message() << ".\n" ;
			throw appcontext::HaltProgramWithReturnCode( -1 ) ;
		}
	}

private:
	typedef std::unordered_map< std::string, std::string > ReadTags ;
	ReadTags m_read_tags ;
	std::size_t m_k ;
	HashSet m_kmers ;

private:
	void unsafe_process() {
		std::size_t const number_of_threads = options().get< std::size_t >( "-threads" ) ;
		uint64_t const multiplicity_threshold = options().get_value< uint64_t >( "-min-kmer-count" ) ;
		std::size_t const max_kmers = options().get< std::size_t >( "-max-kmers" ) ;
		int const base_quality_threshold = options().get< std::size_t >( "-min-base-quality" ) ;
		bool const verbose = options().check( "-verbose" ) ;

		if( options().check( "-read-tags" )) {
			m_read_tags = load_read_tags( options().get< std::string >( "-read-tags" )) ;
		}

		{
			boost::timer timer ;
			double start_time = timer.elapsed() ;

			m_k = load_kmers(
				options().get< std::string >( "-jf" ),
				&m_kmers,
				number_of_threads,
				multiplicity_threshold,
				max_kmers,
				verbose
			) ;

			double end_time = timer.elapsed() ;
			ui().logger()
				<< "++ Ok, "
				<< m_kmers.size() << " "
				<< m_k << "-mers loaded in "
				<< (end_time-start_time)
				<< " seconds:\n" ;
		}

		ui().logger() << "++ Total memory usage is:\n" ;
		ui().logger() << "              (process) : " << std::round(spp::GetProcessMemoryUsed()/1000000.0) << "Mb\n" ;
		ui().logger() << "\n" ;
		{
			ui().logger() << "++ Inspecting reads from \"" << options().get< std::string >( "-reads" ) << "\"...\n" ;
			boost::timer timer ;
			double start_time = timer.elapsed() ;

			// Construct sinks in reverse order.  If outputting to stdout, this makes them output
			// in the right order.
			auto quality_sink = statfile::BuiltInTypeStatSink::open( options().get< std::string >( "-oq" ) ) ;
			auto position_sink = statfile::BuiltInTypeStatSink::open( options().get< std::string >( "-op" ) ) ;
			auto read_sink = statfile::BuiltInTypeStatSink::open( options().get< std::string >( "-or" ) ) ;

			std::auto_ptr< std::istream >
				fastq = genfile::open_text_file_for_input( options().get< std::string >( "-reads" ) ) ;
		
			std::size_t number_of_reads = process_reads(
				*fastq,
				m_kmers,
				m_k,
				base_quality_threshold,
				*read_sink,
				*position_sink,
				*quality_sink
			) ;
			double end_time = timer.elapsed() ;
			ui().logger() << "++ Ok, processed " << number_of_reads << " reads in " << (end_time - start_time) << " seconds.\n" ;
		}
	}

	ReadTags load_read_tags( std::string const& filename ) const {
		ReadTags result ;
		auto source = statfile::BuiltInTypeStatSource::open( filename ) ;
		std::string read_id, read_class ;
		std::size_t count = 0 ;
		while( (*source) >> read_id >> read_class ) {
			auto where = result.find( read_id ) ;
			if( where != result.end() ) {
				if( read_class != where->second ) {
					throw genfile::BadArgumentError(
						"ClassifyKmerApplication::load_read_tags()",
						"filename=\"" + filename + "\"",
						(
							"Read id \""
							+ read_id
							+ "\" appears at least twice with differenct classes."
						)
					) ;
				}
			} else {
				if( !result.insert( std::make_pair( read_id, read_class )).second ) {
					throw genfile::BadArgumentError(
						"ClassifyKmerApplication::load_read_tags()",
						"filename=\"" + filename + "\"",
						(
							"Could not insert read id \""
							+ read_id
							+ "\", for unknown reasons."
						)
					) ;
				}
			}
				
			(*source) >> statfile::ignore_all() ;
		}
		return result ;
	}

	std::size_t load_kmers(
		std::string const& jf_filename,
		HashSet* result,
		std::size_t number_of_threads,
		uint64_t multiplicity_threshold = 0,
		std::size_t max_kmers = std::numeric_limits< std::size_t >::max(),
		bool verbose = false
	) {
		std::ifstream ifs( jf_filename ) ;
		jellyfish::file_header header( ifs ) ;
		std::size_t const k = header.key_len() / 2 ;
		if(verbose) {
			ui().logger()
				<< "++ Loaded header from \"" << jf_filename << "\":\n"
				<< "    size: " << header.size() << "\n"
				<< "    nb_hashes: " << header.nb_hashes() << "\n"
				<< "    key_len: " << header.key_len() << "\n"
				<< "          k: " << k << ".\n" ;
		}

		if( header.format() != binary_dumper::format ) {
			throw genfile::BadArgumentError( "ClassifyKmerApplication::unsafe_process()", "-jf", "Expected a binary-format jellyfish count file." ) ;
		}

		{
			{
				if( number_of_threads < 1 ) {
					throw genfile::BadArgumentError(
						"ClassifyKmerApplication::load_kmers()",
						"number_of_threads",
						"You must supply a value >= 1"
					) ;
				}

				if( (number_of_threads & (number_of_threads - 1) ) != 0 ) {
					throw genfile::BadArgumentError(
						"ClassifyKmerApplication::load_kmers()",
						"number_of_threads",
						"Number of threads must be zero or a power of two."
					) ;
				}

				if( number_of_threads > 32 ) {
					throw genfile::BadArgumentError(
						"ClassifyKmerApplication::load_kmers()",
						"number_of_threads",
						"A maximum of 32 threads are supported."
					) ;
				}
			}
			
			jellyfish::mer_dna::k( k ) ;
			binary_reader reader(ifs, &header);

			{
				std::vector< Queue > queues ;
				std::vector< std::thread > threads ;
				std::atomic< int > quit(0) ;
				ui().logger() << "++ Loading kmers from \"" << jf_filename << "\"\n"
					<< "   ...using " << number_of_threads << " worker threads...\n" ;
			
				// Create worker threads
				for( std::size_t i = 0; i < number_of_threads; ++i ) {
					queues.push_back( Queue( 32768 ) ) ;
					if( verbose ) {
						ui().logger() << "!! Created queue " << i << " at (" << &(queues.back()) << ").\n" ;
					}
				}
				for( std::size_t i = 0; i < number_of_threads; ++i ) {
					threads.push_back(
						std::thread(
							insert_kmer_threaded< HashSet >,
							k,
							&(queues[i]),
							result,
							i,
							&quit
						)
					) ;
					if( verbose ) {
						ui().logger() << "!! Created thread " << i << " at (" << &(threads.back()) << ").\n" ;
					}
				}
				
				// Now read kmers into queues.
				// There is one queue per thread and gets kmers destined for ith
				// hash submap.
				read_kmers_into_queues(
					k,
					multiplicity_threshold,
					reader,
					queues,
					*result,
					max_kmers
				) ;
				
				// Wait for it to finish.
				for( std::size_t i = 0; i < number_of_threads; ++i ) {
					while( queues[i].size_approx() > 0 ) {
#if DEBUG
						std::cerr << "++ Queue[" << i << "] size = " << queues[i].size_approx() << ", waiting..." ;
#endif
						std::this_thread::sleep_for( std::chrono::milliseconds(1)) ;
					}
				}

				ui().logger() << "++ Tidying up...\n" ;
				quit = 1 ;
				std::this_thread::sleep_for( std::chrono::milliseconds(1)) ;
				for( std::size_t i = 0; i < number_of_threads; ++i ) {
					threads[i].join() ;
				}
			}
		}
		return k ;
	}
	
	template< typename Iterator >
	void read_kmers_into_queues(
		unsigned int const k,
		uint64_t const multiplicity_threshold,
		Iterator it,
		std::vector< Queue >& queues,
		HashSet const& set,
		std::size_t const max_kmers = std::numeric_limits< std::size_t >::max()
	) {
		jellyfish::mer_dna::k( k ) ;
		std::size_t count = 0 ;
		{
			auto progress = ui().get_progress_context( "Loading kmers" ) ;
			while( it.next() && count < max_kmers ) {
				if( it.val() >= multiplicity_threshold ) {
					uint64_t const kmer = it.key().get_bits( 0, 2*k ) ;
					std::size_t const hashvalue = set.hash( kmer ) ;
					std::size_t idx = set.subidx( hashvalue ) ;
#if DEBUG > 1
					std::cerr << "++ kmer: " << genfile::kmer::decode_hash( kmer, k )
						<< ", "
						<< std::hex << kmer << std::dec
						<< ": " << "hashvalue: "
						<< hashvalue << ", idx: " << idx << ".\n" ;
#endif
					std::size_t const queue_index = idx % queues.size() ;
					// Queue& queue = queues[ queue_index ] ;
					Queue& queue = queues[ queue_index ] ;
					while( !queue.try_enqueue( kmer )) {
#if DEBUG > 1
						std::cerr << "-- queue full after " << count << " kmers, sleeping...\n" ;
#endif
						std::this_thread::sleep_for( std::chrono::microseconds(10) ) ;
					}
#if DEBUG > 1
					std::cerr << "++ Wrote " << kmer << " to queue " << queue_index << ".\n" ;
#endif
					progress( ++count ) ;
				}
			}
		}
		ui().logger() << "++ read_kmers_into_queues(): Read " << count << " kmers in total.\n" ;
	}
	

	std::size_t process_reads(
		std::istream& input,
		HashSet const& kmers,
		std::size_t k,
		uint64_t base_quality_threshold,
		statfile::BuiltInTypeStatSink& read_sink,
		statfile::BuiltInTypeStatSink& position_sink,
		statfile::BuiltInTypeStatSink& quality_sink
	) {
		std::size_t const number_of_threads = options().get< std::size_t >( "-threads" ) ;

		auto progress = ui().get_progress_context( "Examining reads" ) ;

		std::size_t count = 0 ;

		std::cerr << "process_read(): constructing " << number_of_threads << " worker threads...\n" ;
		std::vector< std::thread > threads ;
		{
			ReadQueue read_queue( 2048 ) ;
			ReadResultQueue read_result_queue( 32768 ) ;
			std::atomic< int > quit(0) ;

			for( std::size_t i = 0; i < number_of_threads; ++i ) {
				threads.push_back(
					std::thread(
						&ClassifyKmerApplication::analyse_reads_threaded,
						this,
						&read_queue,
						&read_result_queue,
						&kmers,
						k,
						base_quality_threshold,
						i,
						&quit
					)
				) ;
			}
			std::cerr << "process_read(): constructing output thread...\n" ;

			std::size_t const length_to_track_at_read_ends = options().get< std::size_t >( "-read-end-length" ) ;
			threads.push_back(
				std::thread(
					&ClassifyKmerApplication::process_read_results,
					this,
					&read_result_queue,
					&read_sink,
					&position_sink,
					&quality_sink,
					length_to_track_at_read_ends,
					&quit
				)
			) ;

			std::cerr << "process_read(): processing reads...\n" ;
			Read read ;
			std::string line ;
			std::size_t l = 0 ;
			while( std::getline( input, line )) {
				switch(l) {
					case 0:
						read.id = line.substr(1,line.size() ) ;
						break ;
					case 1:
						read.sequence = line ;
						break ;
					case 2:
						break ;
					case 3:
						read.qualities = line ;
						break ;
				} ;

				if( (++l) == 4 ) {
					while( !read_queue.try_enqueue( read )) {
#if DEBUG > 1
						std::cerr << "-- queue full after " << count << " reads, sleeping...\n" ;
#endif
						std::this_thread::sleep_for( std::chrono::microseconds(10) ) ;
					}
					
#if DEBUG
					std::cerr << "queued: " << read.id << ".\n" ;
#endif
					++count ;
					progress( count ) ;
					l = 0 ;
				}
			}

			ui().logger() << "++ Done, read " << count << " reads in total.\n" ;

			ui().logger() << "++ Waiting to finish...\n" ;
			while( (read_queue.size_approx() > 0) || (read_result_queue.size_approx() > 0) ) {
				std::this_thread::sleep_for( std::chrono::milliseconds(10)) ;
			}
			std::this_thread::sleep_for( std::chrono::milliseconds(100)) ;
			quit = 1 ;
			std::this_thread::sleep_for( std::chrono::milliseconds(100)) ;
			for( std::size_t i = 0; i < threads.size(); ++i ) {
				threads[i].join() ;
			}
		}

		return count ;
	}
	
	void compute_min_quality(
		genfile::string_utils::slice const& qualities,
		int& min_quality,
		std::size_t& min_quality_at
	) {
		min_quality = std::numeric_limits< int >::max() ;
		for( std::size_t i = 0; i < qualities.size(); ++i ) {
			int quality = get_quality_from_char(qualities[i]) ;
			if( quality < min_quality ) {
				min_quality_at = i ;
				min_quality = quality ;
			}
		}
	}
	
	ReadResult analyse_read(
		Read const& read,
		HashSet const& kmers,
		std::size_t k,
		int const base_quality_threshold
	) {
#if DEBUG
		std::cerr << "analyse_read(): " << read.id << ".\n" ;
#endif
		assert( read.qualities.size() == read.sequence.size() ) ;
		assert( k <= 31 ) ;
		typedef genfile::kmer::KmerHashIterator< std::string::const_iterator > KmerIterator ;

		ReadResult result ;
		result.read = read ;
		result.tag = get_read_tag( read.id ) ;
		result.k = k ;
		KmerIterator kmer_iterator( read.sequence.begin(), read.sequence.end(), k ) ;

		int kmer_min_base_quality = 0 ;
		std::size_t kmer_min_base_quality_at = 0 ;
		double sum_of_base_qualities = 0.0 ;       // sum of PHRED-scale base qualities
		double sum_of_predicted_errors = 0.0 ; // same thing, accumulated on probability not PHRED scale 
		std::vector< uint64_t > number_of_bases_ge_q( 10, 0ul ) ; // bases at q0, q10, q20, q30, q40, q90 and so on.
		bool have_first = false ;
		
		std::size_t i = 0;

		for(
			;
			(i+k) <= read.length();
			++kmer_iterator, ++i
		) {
			// Base quality metrics
			int base_quality = get_quality_from_char(read.qualities[i]) ;
			assert( base_quality <= 93 ) ;
			++result.bases_at_q[std::size_t(base_quality)] ;
			sum_of_base_qualities += double(base_quality) ;
			// bq = -10 log10 (error probability) so use 
			// inverse is 10^(-bq/10)
			sum_of_predicted_errors += double(pow( 10, -base_quality/10.0 )) ;

			for( std::size_t b = 0; b < 9; ++b ) {
				number_of_bases_ge_q[b] += ( base_quality >= (10*b) ) ? 1 : 0 ;
			}

#if DEBUG > 2
			std::cerr
				<< "!! "
				<< read.id << ": " << i << " bq = "
				<< base_quality << ", " << base_quality_threshold
				<< "; k = " << k 
				<< "... adding\n" ;
#endif

			// Kmer metrics
			if( kmer_min_base_quality_at == 0 ) {
				compute_min_quality(
					genfile::string_utils::slice( read.qualities, i, i+k ),
					kmer_min_base_quality,
					kmer_min_base_quality_at
				) ;
			} else {
				--kmer_min_base_quality_at ;
			}
			if( kmer_min_base_quality >= base_quality_threshold ) {
				++(result.number_of_kmers_at_threshold) ;
				uint64_t const hash = kmer_iterator.hash() ;
				uint64_t const reverse_complement_hash = genfile::kmer::reverse_complement( kmer_iterator.hash(), k ) ;
				if( kmers.contains( hash ) || kmers.contains( reverse_complement_hash ) ) {
					if( !have_first ) {
						result.first_solid_kmer_start = i ;  // 0-based, half-closed
						have_first = true ;
					}
					++(result.number_of_solid_kmers_at_threshold) ;
					result.last_solid_kmer_end = i+k ; // 0-based, half-closed
				} else {
					result.error_positions.push_back(i) ;
#if DEBUG
					std::cerr << "!! kmer not in hash:" << kmer_iterator.to_string() << ":\n"
						<< "    (fwd): " << genfile::kmer::decode_hash( hash, k ) << ": " << hash << "\n"
						<< "    (rev): " << genfile::kmer::decode_hash( reverse_complement_hash, k ) << ".\n" ;
#endif
				}
			}
		}

#if DEBUF	
		std::cerr << "analyse_read(): " << read.id << ": capturing quality metrics...\n" ;
#endif
		// The above loop only covers kmer start positions,
		// now iterate to the end of the read.
		for(
			;
			i < read.length();
			++i
		) {
			int base_quality = get_quality_from_char(read.qualities[i]) ;
			assert( base_quality <= 93 ) ;
			++result.bases_at_q[std::size_t(base_quality)] ;
			sum_of_base_qualities += double(base_quality) ;
			sum_of_predicted_errors += double(pow( 10, -base_quality/10.0 )) ;
			for( std::size_t b = 0; b < 9; ++b ) {
				number_of_bases_ge_q[b] += ( base_quality >= (10*b) ) ? 1 : 0 ;
			}
#if DEBUG > 2
			std::cerr << "!!! " << read.id << ": " << i << " bq = " << base_quality << "... adding\n" ;
#endif
		}
		
		result.mean_base_quality = sum_of_base_qualities / result.read.length() ;
		result.mean_base_quality2 = -10 * log10( sum_of_predicted_errors / result.read.length()) ;
		for( std::size_t b = 0; b < 9; ++b ) {
			result.number_of_bases_ge_q[b] = number_of_bases_ge_q[b] ;
		}
#if DEBUG
		std::cerr << "analyse_read(): " << read.id << ": finished.\n" ;
#endif
		return result ;
	}

	std::string get_read_tag( std::string const& read_id ) const {
		std::string result = "NA" ;
		ReadTags::const_iterator where
			= m_read_tags.find( read_id ) ;
		if( where != m_read_tags.end() ) {
			result = where->second ;
		}
		return result ;
	}
	
	void analyse_reads_threaded(
		ReadQueue* read_queue,
		ReadResultQueue* result_queue,
		HashSet const* kmers,
		std::size_t k,
		uint64_t base_quality_threshold,
		std::size_t const thread_index,
		std::atomic< int >* quit
	) {
#if DEBUG
		std::cerr << "(thread " << thread_index << "): Starting...\n" ;
#endif	
		Read read ;
		while( !(*quit) ) {
			bool popped = read_queue->try_dequeue( read ) ;
			if( popped ) {
				ReadResult const result = analyse_read( read, *kmers, k, base_quality_threshold ) ;
				while( !result_queue->try_enqueue( result )) {
					std::this_thread::sleep_for( std::chrono::microseconds(10) ) ;
				}
			} else {
				// nothing to pop, sleep to allow queue to fill.
				std::this_thread::sleep_for( std::chrono::microseconds(10) ) ;
			}
		}
	}

	void process_read_results(
		ReadResultQueue* result_queue,
		statfile::BuiltInTypeStatSink* read_sink,
		statfile::BuiltInTypeStatSink* read_position_sink,
		statfile::BuiltInTypeStatSink* quality_sink,
		std::size_t const length_to_track_at_read_ends,
		std::atomic< int >* quit
	) {
		bool const include_tags = options().check( "-read-tags" ) ;
		write_per_read_result_header( read_sink, include_tags ) ;
		
		std::map< std::string, ReadEndMetrics > read_start_metrics ;
		std::map< std::string, ReadEndMetrics > read_end_metrics ;
		std::map< std::string, std::vector< uint64_t > > quality_metrics ;

		std::size_t count = 0 ;
		
		ReadResult result ;
		while( !(*quit) ) {
			bool popped = result_queue->try_dequeue( result ) ;
			if( popped ) {
				process_read_result( result, read_sink, include_tags ) ;
				if( result.read.length() >= 2 * length_to_track_at_read_ends ) {
					accumulate_read_end_metrics(
						result,
						length_to_track_at_read_ends,
						read_start_metrics,
						read_end_metrics
					) ;
				}
				accumulate_quality_metrics(
					result,
					quality_metrics
				) ;
				++count ;
			} else {
				// nothing to pop, sleep to allow queue to fill.
				std::this_thread::sleep_for( std::chrono::microseconds(10) ) ;
			}
		}
		ui().logger() << "++ Outputting read-end metrics...\n" ;
		process_read_end_metrics(
			read_start_metrics,
			read_end_metrics,
			read_position_sink
		) ;

		ui().logger() << "++ Outputting quality metrics...\n" ;
		output_quality_metrics(
			quality_metrics,
			quality_sink
		) ;
		ui().logger() << "++ Success.\n" ;
	}
	
	void process_read_result(
		ReadResult const& read_result,
		statfile::BuiltInTypeStatSink* output,
		bool const include_tags
	) {
		write_per_read_result(
			read_result,
			output,
			include_tags
		) ;
	}
	
	void write_per_read_result_header(
		statfile::BuiltInTypeStatSink* output,
		bool const include_tags
	) {
		(*output)
			| "read_id" ;
		if( include_tags ) {
			(*output) | "read_tag" ;
		}
		(*output)
			| "read_length"
			| "mean_base_quality"
			| "mean_base_quality2"
			| "kmer_k"
			| "number_of_kmers_at_threshold"
			| "number_of_solid_kmers_at_threshold"
			| "first_solid_kmer_start"
			| "last_solid_kmer_end"
			| "n_bases_at_q10"
			| "n_bases_at_q20"
			| "n_bases_at_q30"
			| "n_bases_at_q40"
			| "n_bases_at_q50"
			| "n_bases_at_q60"
			| "n_bases_at_q70"
			| "n_bases_at_q80"
			| "n_bases_at_q90"
		;
	}

	void write_per_read_result(
		ReadResult const& read_result,
		statfile::BuiltInTypeStatSink* output,
		bool include_tags
	) {
#if DEBUG
		std::cerr << "++ write_per_read_result(): " << result.id << ".\n" ;
#endif
		(*output)
			<< read_result.read.id ;
		if( include_tags ) {
			(*output) << read_result.tag ;
		}
		(*output)
			<< uint64_t(read_result.read.length())
			<< read_result.mean_base_quality
			<< read_result.mean_base_quality2
			<< read_result.k
			<< read_result.number_of_kmers_at_threshold
			<< read_result.number_of_solid_kmers_at_threshold
			<< (read_result.first_solid_kmer_start+1) 			// convert to 1-based, closed
			<< (read_result.last_solid_kmer_end) 				// 1-based, closed.
			<< read_result.number_of_bases_ge_q[1] 				// q10
			<< read_result.number_of_bases_ge_q[2] 				// q20
			<< read_result.number_of_bases_ge_q[3] 				// q30
			<< read_result.number_of_bases_ge_q[4] 				// q40
			<< read_result.number_of_bases_ge_q[5] 				// q50
			<< read_result.number_of_bases_ge_q[6] 				// q60
			<< read_result.number_of_bases_ge_q[7] 				// q70
			<< read_result.number_of_bases_ge_q[8] 				// q80
			<< read_result.number_of_bases_ge_q[9] 				// q90
			<< statfile::end_row() ;
	}

	void accumulate_read_end_metrics(
		ReadResult const& read_result,
		std::size_t length_to_track,
		std::map< std::string, ReadEndMetrics >& read_start_metrics,
		std::map< std::string, ReadEndMetrics >& read_end_metrics
	) {
		typedef std::map< std::string, ReadEndMetrics >::iterator Iterator ;
		Iterator
			start_where = read_start_metrics.find( read_result.tag ),
			end_where = read_end_metrics.find( read_result.tag ) ;
		if( start_where == read_start_metrics.end() ) {
			auto inserted = read_start_metrics.emplace(
				read_result.tag,
				ReadEndMetrics( length_to_track ) 
			) ;
			start_where = inserted.first ;
		}
		if( end_where == read_end_metrics.end() ) {
			auto inserted = read_end_metrics.emplace(
				read_result.tag,
				ReadEndMetrics( length_to_track ) 
			) ;
			end_where = inserted.first ;
		}
		accumulate_read_end_metrics(
			read_result,
			start_where->second,
			end_where->second
		) ;
	}

	void accumulate_read_end_metrics(
		ReadResult const& read_result,
		ReadEndMetrics& read_start_metrics,
		ReadEndMetrics& read_end_metrics
	) {
		// Accumulate where errors lie in the read.
		// We only count reads with at least twice the required length.
		// This way we don't capture end-of-read effects in the start-of-read accounting,
		// and vice-versa.
		std::size_t const end_length = read_start_metrics.length() ;
		for( std::size_t i = 0; i < end_length; ++i ) {
			std::size_t end_of_read_i = read_result.read.length() - end_length + i ;

			// accumulate read counts
			++read_start_metrics.counts[i] ;
			++read_end_metrics.counts[i] ;
			
			// accumulate bases
			switch( read_result.read.sequence[i] ) {
				case 'A':
				case 'a':
					++read_start_metrics.A[i] ;
					break ;
				case 'C':
				case 'c':
					++read_start_metrics.C[i] ;
					break ;
				case 'G':
				case 'g':
					++read_start_metrics.G[i] ;
					break ;
				case 'T':
				case 't':
					++read_start_metrics.T[i] ;
					break ;
				default:
					break ;
			}

			switch( read_result.read.sequence[end_of_read_i] ) {
				case 'A':
				case 'a':
					++read_end_metrics.A[i] ;
					break ;
				case 'C':
				case 'c':
					++read_end_metrics.C[i] ;
					break ;
				case 'G':
				case 'g':
					++read_end_metrics.G[i] ;
					break ;
				case 'T':
				case 't':
					++read_end_metrics.T[i] ;
					break ;
				default:
					break ;
			}
			
			// accumulate mean base qualities.
			read_start_metrics.qualities[i] += get_quality_from_char( read_result.read.qualities[i] ) ;
			read_end_metrics.qualities[i] += get_quality_from_char( read_result.read.qualities[end_of_read_i] ) ;
		}
		
		// accumulate errors
		for( std::size_t i = 0; i < read_result.error_positions.size(); ++i ) {
			std::size_t const pos = read_result.error_positions[i] ;
			// Example:
			//   = =               kmer pos = 1, k = 2
			// - - - - - - - - - - sequence length = 10
			// 0 1 2 3 4 5 6 7 8 9  
			// [         ]  end_length = 4 capturing 4+k-1 = 5 bases in total
			if( pos < end_length ) {
				++read_start_metrics.errors[pos] ;
			}
			// Example:
			//                 = = kmer pos = 8, k = 2
			// - - - - - - - - - - sequence length = 10
			// 0 1 2 3 4 5 6 7 8 9  
			//          [         ]  end_length = 4 capturing 4+k-1 = 5 bases in total
			// hence we need to track all kmer start positions with pos + end_length + k > sequence length
			// and then pos maps to index (pos + (end_length + k - 1) - sequence length) in the read end metrics.
			// e.g. in this case 8+4+2-1-10 = 3.
			if( (pos + end_length + read_result.k ) > read_result.read.length() ) {
				std::size_t const idx = (pos + end_length + read_result.k) - 1 - read_result.read.length() ;
#if DEBUG > 1
				std::cerr
					<<   "        pos: " << pos
					<< "\n end_length: " << end_length 
					<< "\n          k: " << read_result.k
					<< "\n     length: " << read_result.read.length()
					<< "\n      index: " << idx
					<< "\n" ;
#endif
				++read_end_metrics.errors[idx] ;
			}
		}
	}
	
	void process_read_end_metrics(
		std::map< std::string, ReadEndMetrics > const& read_start_metrics,
		std::map< std::string, ReadEndMetrics > const& read_end_metrics,
		statfile::BuiltInTypeStatSink* read_position_results
	) const {
		output_read_end_metrics(
			read_start_metrics,
			read_end_metrics,
			read_position_results
		) ;
	}

	void output_read_end_metrics(
		std::map< std::string, ReadEndMetrics > const& read_start_metrics,
		std::map< std::string, ReadEndMetrics > const& read_end_metrics,
		statfile::BuiltInTypeStatSink* sink
	) const {
		bool const have_tags = options().check( "-read-tags" ) ;
		if( have_tags ) {
			(*sink) | "read_tag" ;
		}

		(*sink)
			| "start_or_end"
			| "position"
			| "number_of_errors"
			| "number_of_reads"
			| "sum_of_base_qualities"
			| "A"
			| "C"
			| "G"
			| "T"
		;

		for( auto& kv: read_start_metrics ) {
			ReadEndMetrics const& start_metrics = kv.second ;
			for( std::size_t i = 0; i < start_metrics.length(); ++i ) {
				if( have_tags ) {
					(*sink) << kv.first ;
				}
				(*sink)
					<< "start"
					<< uint64_t(i+1)
					<< start_metrics.errors[i]
					<< start_metrics.counts[i]
					<< start_metrics.qualities[i]
					<< start_metrics.A[i]
					<< start_metrics.C[i]
					<< start_metrics.G[i]
					<< start_metrics.T[i]
					<< statfile::end_row() ;
			}
		}
	
		for( auto& kv: read_end_metrics ) {
			ReadEndMetrics const& end_metrics = kv.second ;
			for( std::size_t i = 0; i < end_metrics.length(); ++i ) {
				if( have_tags ) {
					(*sink) << kv.first ;
				}
				(*sink)
					<< "end"
					// 0 maps to -end_of_read_errors.size()
					<< (-int64_t(end_metrics.length()) + int64_t(i) )
					<< end_metrics.errors[i]
					<< end_metrics.counts[i]
					<< end_metrics.qualities[i]
					<< end_metrics.A[i]
					<< end_metrics.C[i]
					<< end_metrics.G[i]
					<< end_metrics.T[i]
					<< statfile::end_row() ;
			}
		}
	}

	void accumulate_quality_metrics(
		ReadResult const& read_result,
		std::map< std::string, std::vector< uint64_t > >& quality_metrics
	) {
		typedef std::map< std::string, std::vector< uint64_t > >::iterator Iterator ;
		Iterator
			where = quality_metrics.find( read_result.tag ) ;
		if( where == quality_metrics.end() ) {
			auto inserted = quality_metrics.emplace(
				read_result.tag,
				read_result.bases_at_q
			) ;
			assert( inserted.second ) ;
			assert( inserted.first->second.size() == 94 ) ;
		} else {
			for( std::size_t b = 0; b < 94; ++b ) {
				(where->second)[b] += read_result.bases_at_q[b] ;
			}
		}
	}

	void output_quality_metrics(
		std::map< std::string, std::vector< uint64_t > > const& metrics,
		statfile::BuiltInTypeStatSink* sink
	) const {
		bool const have_tags = options().check( "-read-tags" ) ;
		if( have_tags ) {
			(*sink) | "read_tag" ;
		}
		(*sink)
			| "base_quality"
			| "count"
		;

		for( auto& kv: metrics ) {
			std::vector< uint64_t > const& counts = kv.second ;
			for( std::size_t b = 0; b < counts.size(); ++b ) {
				if( have_tags ) {
					(*sink) << kv.first ;
				}
				(*sink)
					<< int64_t(b)
					<< counts[b]
					<< statfile::end_row() ;
			}
		}
	}
} ;


int main( int argc, char** argv )
{
	std::ios_base::sync_with_stdio( false ) ;
	try {
		ClassifyKmerApplication app( argc, argv ) ;
		app.run() ;
	}
	catch( appcontext::HaltProgramWithReturnCode const& e ) {
		return e.return_code() ;
	}
	return 0 ;
}

